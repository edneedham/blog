---
date: '2025-11-05'
title: 'Ephemeral Environments'
tags: ['Testing', 'Infrastructure', 'En Punto']
category: 'dev-diary'
language: 'en'
---

The last blog post was left on a bit of cliffhanger because even though we had solved the issues with 
the tools (Playwright, Mock Service Worker) we could already start to see problems with scale and 
the current testing infrastructure. Well, this post aims to address that.

If you want to read the previous post in this series, [click me](/posts/end-to-end-testing).

As a reminder, we made the tradeoff to dockerise the end to end testing infrastructure so that the 
testing environment behaves somewhat like a mirror to production, allowing end to end tests to act as if 
they were a real user. And to avoid the issues we had with getting playwright to intercept server to 
server communication from Node.js to the Go api. But there are issues with our approach, it was a tradeoff
after all. 

Every time we create a new Pull Request we trigger unit and integration tests. Fine. We also can trigger
end to end tests to run if we want. Also fine. The app deploys and it can be manually tested too. 

In our current solution, manual testing is coming last in our workflow, after the code has been 
merged. 

This creates several problems, firstly tests don't necessarily capture the intended behaviour, only what 
the person writing those tests thinks the behaviour should be, at best. Secondly, we have to wait for 
the tests to run and pass before it is deployed and merging is a pre-requisite for deployment.
Then we have to wait for manual testing, which could fail. In this time, development stops.

So we have two issues, the workflow order and having a long-lasting staging environment. If all deployments
go to the same place then there is going to be a queue, right?

Ephemeral environments have been our solution for this problem, and it actually solves both issues. I think
it is unreasonable for code that fails automated tests to be pushed to a long-standing environment just so
it can be manually tested, so automated tests should still come before manual. We don't want to merge failing
code. However, if it passes automated testing but fails manual testing it still fails.
But it is faster (on average) to fix a failing automated test than a manual one because it requires no 
communication between people. 

Anyway, by adopting ephemeral environments we have been able to change the order of the workflow, we now deploy
first, not last. But what if the automated tests fail? Well, we still do unit and integration tests, just
we don't run end to end tests until the Pull Request receives the ready-to-deploy label (manual testing passes).

Doesn't this seem counter to what I said before? We don't actually merge until everything passes, before 
we merged in order to deploy, potentially failing manual tests. Now we deploy with an environment that is
created just for the changes in the Pull Request. With this we don't dirty any long-lasting branches and 
then need to rollback, we just create an environment from the workflow that is then destroyed once the 
PR receives the ready-to-merge label.

The frontend is deployed on Vercel which automatically creates independent previews on every deploy. Thanks,
Vercel. The api is deployed on fly.io so we create a new app for each new PR within the workflow. While the
workflow also integrates with a Neon github app to create a new database branch on every PR.

Because every change is now a new deployment code can continue to be developed rather than waiting for end
to end or manual tests to pass. 

We have also had to adapt our seeder because we don't want to be running our full seeder every time we 
create a new PR to populate the preview database branch. Instead, we run a very small seeder, just enough
for the manual tests.
